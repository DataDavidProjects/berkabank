{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "input_bucket_path = \"gs://berkabank/production/data/\"\n",
    "import joblib\n",
    "\n",
    "bucket_name = \"berkabank\"\n",
    "source_blob_name = \"production/artifacts/model/berkamodel.joblib\"\n",
    "destination_file_name = \"./berkamodel.joblib\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(source_blob_name)\n",
    "blob.download_to_filename(destination_file_name)\n",
    "\n",
    "joblib.load(destination_file_name)\n",
    "from typing import List\n",
    "import joblib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelPipeline:\n",
    "    \"\"\"Pipeline for prediction\n",
    "    Args:\n",
    "        model_path (str): Path to the model file\n",
    "\n",
    "    Attributes:\n",
    "        model_path (str): Path to the model file\n",
    "\n",
    "    Methods:\n",
    "        load_model(): Load the model from disk\n",
    "        processing(data): Preprocess the data\n",
    "        inference(data): Predict using the model\n",
    "        postprocessing(prediction): Postprocess the prediction\n",
    "        predict(data): Predict using the model\n",
    "\n",
    "    Returns:\n",
    "        output: Prediction output\n",
    "    \"\"\"\n",
    "\n",
    "    model_path: str = \"./model/model.joblib\"\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load model from disk\"\"\"\n",
    "        model = joblib.load(self.model_path)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def processing(self, data):\n",
    "        \"\"\"Preprocess data\"\"\"\n",
    "        return data\n",
    "\n",
    "    def inference(self, data):\n",
    "        \"\"\"Predict using the model\"\"\"\n",
    "        prediction = self.model.predict_proba(data)\n",
    "        return prediction\n",
    "\n",
    "    def postprocessing(self, prediction):\n",
    "        \"\"\"Postprocess prediction\"\"\"\n",
    "        return prediction\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict using the model\"\"\"\n",
    "\n",
    "        data = self.processing(data)\n",
    "        prediction = self.inference(data)\n",
    "        output = self.postprocessing(prediction)\n",
    "        return output\n",
    "\n",
    "import os\n",
    "from flask import Flask, jsonify, request, json\n",
    "import pandas as pd\n",
    "# from predict import ModelPipeline\n",
    "from google.cloud import storage\n",
    "import joblib\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "# Load ENV\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Define the details component\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "REGION = os.environ.get(\"REGION\")\n",
    "BUCKET_NAME = os.environ.get(\"BUCKET_NAME\")\n",
    "MODEL_NAME = \"berkamodel\"\n",
    "\n",
    "# Download model from cloud storage\n",
    "# source_blob_name = f\"production/artifacts/model/{MODEL_NAME}.joblib\"\n",
    "# destination_file_name = \"./model.joblib\"\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.bucket(BUCKET_NAME)\n",
    "# blob = bucket.blob(source_blob_name)\n",
    "# blob.download_to_filename(destination_file_name)\n",
    "# joblib.load(destination_file_name)\n",
    "\n",
    "# Start Flask Server\n",
    "app = Flask(__name__)\n",
    "# AIP_HEALTH_ROUTE = os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\")\n",
    "# AIP_PREDICT_ROUTE = os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\")\n",
    "\n",
    "\n",
    "@app.route(\"/health\")\n",
    "def health():\n",
    "    \"\"\"Health endpoint.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        response: health response\n",
    "    \"\"\"\n",
    "    return \"OK\", 200\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\", \"GET\"])\n",
    "def predict():\n",
    "    \"\"\"Predict endpoint.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        request (post): post request with instances in body\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        response: prediction response\n",
    "    \"\"\"\n",
    "\n",
    "    predictor = ModelPipeline(\"./berkamodel.joblib\")\n",
    "\n",
    "    features_names = predictor.model.feature_names_in_.tolist()\n",
    "    instances = request.get_json()[\"instances\"]\n",
    "    data = pd.DataFrame(instances)[features_names]\n",
    "    results = predictor.predict(data=data) # tobe score method\n",
    "\n",
    "    # Format Vertex AI prediction response\n",
    "    predictions = [\n",
    "        {\"probability_negative\": result[0], \"probability_positive\": result[1]}\n",
    "        for result in results\n",
    "    ]\n",
    "\n",
    "    return jsonify({\"predictions\": predictions})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(\n",
    "        host=\"0.0.0.0\",\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ModelPipeline(\"./berkamodel.joblib\")\n",
    "# random model test\n",
    "data = np.random.randint(low=0, high=100, size=(10, 1))\n",
    "df = pd.DataFrame(data, columns=['n_transactions'])\n",
    "predictor.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"account_id\"] = df.index\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(df.set_index(\"account_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
